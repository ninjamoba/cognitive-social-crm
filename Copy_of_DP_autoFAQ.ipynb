{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DP:autoFAQ.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninjamoba/cognitive-social-crm/blob/master/Copy_of_DP_autoFAQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7fbDvssiBrV"
      },
      "source": [
        "# Simple intent recognition and question answering with DeepPavlov\n",
        "\n",
        "This notebook consists of the [DeepPavlov](https://github.com/deepmipt/DeepPavlov) code snippets. The snippets show how to interact with the text classification models that were specifically developed to be effective when training data is limited. The popular use case scenario for these models is to classify user utterances into one of the FAQ questions and retrieve the corresponding answer (autoFAQ models). As a testbed, we use the students’ FAQ from the [MIPT website](https://mipt.ru/english/). The FAQ contains the most popular first-year students' questions with corresponding answers.\n",
        "The framework allows you to train models, fine-tune hyperparameters, and test models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_hoGWCfhxLh"
      },
      "source": [
        "# Requirements\n",
        "\n",
        "First, install all required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhvF6Petcd0r"
      },
      "source": [
        "!pip install -q deeppavlov"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVWji-oNOl2I"
      },
      "source": [
        "# Model Description\n",
        "\n",
        "DeepPavlov contains several text classification models that work well on few training pairs. All the models are based on two major text representations: fastText word embeddings and tf-idf representation. The models described in the separated configuration files under the [config/faq](https://github.com/deepmipt/DeepPavlov/tree/master/deeppavlov/configs/faq) folder. The config file consists of four main sections: **dataset_reader**, **dataset_iterator**, **chainer**, and **train**.\n",
        "\n",
        "The **dataset_iterator** specifies how to split the data into train, valid, test sets. The **chainer** section of the configuration files contains a pipeline of the required components to interact with the models, i.e. tokenizer, lemmatizer, tf-idf vectorizer, and others. The tokenizer splits a string into tokens, lemmatizer converts all tokens into lemmas. The tf-idf vectorizer transforms the lemmas into tf-idf vectors. The component’s input and output are defined in the **in** and **out** keys correspondingly.\n",
        "\n",
        "Run the following cell to see a [configuration file](https://github.com/deepmipt/DeepPavlov/blob/master/deeppavlov/configs/faq/tfidf_logreg_en_faq.json) based on logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg9RZOW_KNto"
      },
      "source": [
        "%load https://raw.githubusercontent.com/deepmipt/DeepPavlov/master/deeppavlov/configs/faq/tfidf_logreg_en_faq.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1hsQECYf5yX"
      },
      "source": [
        "# Interacting with the model\n",
        "\n",
        "The DeepPavlov framework contains several models pre-trained on the aforementioned MIPT FAQ corpus. The files with the pre-trained models defined in the **metadata: download** section of the model's configuration file. You can interact with the model by running it from the command line with **interact** parameter and the name of  the model's configuration file (-d indicates to download all required files). But first, **install** all the model requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UydjsTwLfryF"
      },
      "source": [
        "!python -m deeppavlov install tfidf_logreg_en_faq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_YQYuj2_Xpx"
      },
      "source": [
        "!python -m deeppavlov interact tfidf_logreg_en_faq -d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48TD-v-FO25O"
      },
      "source": [
        "Alternatively, you can **build_model** from the Python code as on the example below. In addition, please make sure that you can navigate the configuration files by using Autocomplete (Tab key) with **configs** module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "wMZqyzBYc2eV"
      },
      "source": [
        "from deeppavlov import configs\n",
        "from deeppavlov.core.common.file import read_json\n",
        "from deeppavlov.core.commands.infer import build_model\n",
        "\n",
        "faq = build_model(configs.faq.tfidf_logreg_en_faq, download = True)\n",
        "a = faq([\"I need help\"])\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ_Uqj7K52En"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "You can train a model by running the library with **train** parameter, wherein the model will be trained on the dataset defined in the dataset_reader section of the configuration file. If **metrics** key along with either **validate_best** or **test_best** is defined in the train section, the model will be validated/tested on the corresponding set in the dataset_iterator section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfOBVt_U5zE4"
      },
      "source": [
        "!python -m deeppavlov train tfidf_logreg_en_faq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93GqENh-PAHc"
      },
      "source": [
        "Let's modify the training data and retrain the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEXzImAIPGEt"
      },
      "source": [
        "%%bash\n",
        "wget -q http://files.deeppavlov.ai/faq/school/faq_school_en.csv -O faq.csv\n",
        "echo \"What's DeepPavlov?, DeepPavlov is an open-source conversational AI library\" >> faq.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN-Yb8Vd4oD6"
      },
      "source": [
        "from deeppavlov import configs, train_model\n",
        "\n",
        "model_config = read_json(configs.faq.tfidf_logreg_en_faq)\n",
        "model_config[\"dataset_reader\"][\"data_path\"] = \"/content/faq.csv\"\n",
        "model_config[\"dataset_reader\"][\"data_url\"] = None\n",
        "faq = train_model(model_config)\n",
        "a = faq([\"tell me about DeepPavlov\"])\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s2RLyyYUogN"
      },
      "source": [
        "# About Us\n",
        "\n",
        "We are iPavlov, our story started in 2017 when we decided to build a conversational AI framework that on the one hand will contain all required NLP components to build chatbots and on the other hand will be easy to use. Our work resulted in releasing DeepPavlov library. Our lab at MIPT is honored with Facebook AI Academic Partnership and NVIDIA GPU Research Center status. We successfully combine research and extreme coding in our week-long DeepHack.me hackathons — DeepHack.Game, DeepHack.Q&A and DeepHack.RL. We serve a global AI community by organizing NIPS Conversational Challenge to evaluate state-of-the-art techniques in the field of dialog systems and collect open source dialog datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yom6JD53QcXk"
      },
      "source": [
        "# Useful links\n",
        "\n",
        "[DeepPavlov repository](https://github.com/deepmipt/DeepPavlov)\n",
        "\n",
        "[DeepPavlov demo page](https://demo.ipavlov.ai)\n",
        "\n",
        "[DeepPavlov documentation](https://docs.deeppavlov.ai)"
      ]
    }
  ]
}